{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kitchen training - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T12:06:23.577425Z",
     "start_time": "2019-11-07T12:06:23.573439Z"
    }
   },
   "outputs": [],
   "source": [
    "import gdown\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "OD_PATH = '/models/research/object_detection/'\n",
    "MAIN_PATH = '/happy-walrus/models/version_3/'\n",
    "IMG_PATH = MAIN_PATH + 'images/'\n",
    "MODEL_NAME = 'autoaugm_inception_resnet_v2_atrous_coco/'\n",
    "TRAIN_PATH = MAIN_PATH + 'training/' + MODEL_NAME\n",
    "EVAL_PATH = MAIN_PATH + 'evaluation/' + MODEL_NAME\n",
    "GRAPH_PATH = MAIN_PATH + 'inference_graph/' + MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "- [Tensorflow Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection)\n",
    "- [Tensorflow Object Detection API tutorial](https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store all images and annotations in a single folder, zip the folder and store it in Google Drive under `safety/data/images/version_X` (replace the `X`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T12:06:30.407521Z",
     "start_time": "2019-11-07T12:06:29.516282Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=16PHxTQs2JW8Zq1HpsscBLcQuWcFGG_gR\n",
      "To: /happy-walrus/models/version_3/version_3.zip\n",
      "580MB [00:02, 205MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files unzipped: 5757\n",
      "XML files: 2881\n",
      "JPG files: 2876\n"
     ]
    }
   ],
   "source": [
    "# Download images\n",
    "file_id = \"16PHxTQs2JW8Zq1HpsscBLcQuWcFGG_gR\"\n",
    "file_name = \"version_3.zip\"\n",
    "gdown.download(url=\"https://drive.google.com/uc?id=\" + file_id,\n",
    "               output=MAIN_PATH + file_name,\n",
    "               quiet=False)\n",
    "\n",
    "# Unzip images\n",
    "os.mkdir(IMG_PATH)\n",
    "!unzip -q {MAIN_PATH}{file_name} -d {IMG_PATH}\n",
    "!rm {MAIN_PATH}version_3.zip\n",
    "!cd {MAIN_PATH}images && ls | grep \"(1)\" | xargs -d\"\\n\" rm\n",
    "files_unzipped = os.listdir(MAIN_PATH + 'images/')\n",
    "\n",
    "# Print results\n",
    "print('Files unzipped:', len(files_unzipped))\n",
    "print('XML files:', len([file for file in files_unzipped if re.match('.*\\.xml', file)]))\n",
    "print('JPG files:', len([file for file in files_unzipped if re.match('.*\\.jpg', file)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly split data into train and test:\n",
    "```\n",
    "|-images\n",
    "    |- train\n",
    "    |- test\n",
    "```\n",
    "where the last level folders will contain both the images and annotations (.xml) files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed files: 69\n",
      "Number of errors: 0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Create train and test folders in images/\n",
    "os.mkdir(IMG_PATH + 'train')\n",
    "os.mkdir(IMG_PATH + 'test')\n",
    "\n",
    "# Get list of all files that have annotations and split into train and test\n",
    "jpg_files = [file[:-4] for file in os.listdir(IMG_PATH) if re.search('.*\\.jpg$', file)]\n",
    "files = [file[:-4] for file in os.listdir(IMG_PATH) if re.search('.*\\.xml$', file) and file[:-4] in jpg_files]\n",
    "files = [file for file in files if not re.search('open_oven', file)]\n",
    "files_train, files_test = train_test_split(files, train_size=0.9, test_size=0.1, random_state=2014)\n",
    "\n",
    "# Move images to train or test folders\n",
    "errors = []\n",
    "for file in files_train:\n",
    "    try:\n",
    "        shutil.move(IMG_PATH + file + '.xml', IMG_PATH + 'train/' + file + '.xml')\n",
    "        shutil.move(IMG_PATH + file + '.jpg', IMG_PATH + 'train/' + file + '.jpg')\n",
    "    except Exception as err:\n",
    "        errors.append([file, err])\n",
    "\n",
    "for file in files_test:\n",
    "    try:\n",
    "        shutil.move(IMG_PATH + file + '.xml', IMG_PATH + 'test/' + file + '.xml')\n",
    "        shutil.move(IMG_PATH + file + '.jpg', IMG_PATH + 'test/' + file + '.jpg')\n",
    "    except Exception as err:\n",
    "        errors.append([file, err])\n",
    "\n",
    "# Remove remaining images\n",
    "files_rm = [file for file in os.listdir(IMG_PATH) if re.search('.*\\.jpg|.*\\.xml', file)]\n",
    "for file in files_rm:\n",
    "    os.remove(IMG_PATH + file)\n",
    "        \n",
    "# Print errors\n",
    "print(\"Removed files:\", len(files_rm))\n",
    "print(\"Number of errors:\", len(errors))\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate training and test data to `./images/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted xml to csv.\n",
      "Successfully converted xml to csv.\n",
      "\n",
      "filename,width,height,class,xmin,ymin,xmax,ymax\n",
      "kitchen_apartment.NM.048.jpg,1280,720,stool,786,621,959,678\n",
      "kitchen_apartment.NM.048.jpg,1280,720,oven,749,466,886,580\n",
      "kitchen_apartment.NM.048.jpg,1280,720,countertop,356,422,893,460\n",
      "purple_kitchen.EH.025.jpg,1000,717,stove,247,358,384,368\n",
      "\n",
      "Unique classes: ['cabinet', 'chair', 'countertop', 'dishwasher', 'outlet', 'oven', 'sofa', 'stool', 'stove', 'utensil']\n",
      "\n",
      "Train images: 2539\n",
      "Train annotations: 18,949\n",
      "Annotations per class (training):\n",
      "countertop    4622\n",
      "cabinet       3543\n",
      "chair         2669\n",
      "outlet        2327\n",
      "stove         1995\n",
      "utensil       1459\n",
      "oven           960\n",
      "stool          692\n",
      "dishwasher     584\n",
      "sofa            98\n",
      "Name: class, dtype: int64\n",
      "\n",
      "Test images: 284\n",
      "Test annotations: 2,058\n",
      "\n",
      "Label id mapping:\n",
      "item{\n",
      "\tid: 1\n",
      "\tname: 'cabinet'\n",
      "}\n",
      "item{\n",
      "\tid: 2\n",
      "\tname: 'chair'\n",
      "}\n",
      "item{\n",
      "\tid: 3\n",
      "\tname: 'countertop'\n",
      "}\n",
      "item{\n",
      "\tid: 4\n",
      "\tname: 'dishwasher'\n",
      "}\n",
      "item{\n",
      "\tid: 5\n",
      "\tname: 'outlet'\n",
      "}\n",
      "item{\n",
      "\tid: 6\n",
      "\tname: 'oven'\n",
      "}\n",
      "item{\n",
      "\tid: 7\n",
      "\tname: 'sofa'\n",
      "}\n",
      "item{\n",
      "\tid: 8\n",
      "\tname: 'stool'\n",
      "}\n",
      "item{\n",
      "\tid: 9\n",
      "\tname: 'stove'\n",
      "}\n",
      "item{\n",
      "\tid: 10\n",
      "\tname: 'utensil'\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Generate training and test data\n",
    "!python {OD_PATH}xml_to_csv.py\n",
    "print(\"\")\n",
    "!head {IMG_PATH}train_labels.csv -n 5\n",
    "\n",
    "# Filter ladders out, fix utensil(s)\n",
    "print(\"\")\n",
    "train_csv = pd.read_csv(IMG_PATH + 'train_labels.csv')\n",
    "train_csv = train_csv[train_csv['class'] != 'ladder']\n",
    "train_csv['class'] = train_csv['class'].map(lambda x: 'utensil' if x == 'utensils' else x)\n",
    "train_csv.to_csv(IMG_PATH + 'train_labels.csv')\n",
    "\n",
    "test_csv = pd.read_csv(IMG_PATH + 'test_labels.csv')\n",
    "test_csv = test_csv[test_csv['class'] != 'ladder']\n",
    "test_csv['class'] = test_csv['class'].map(lambda x: 'utensil' if x == 'utensils' else x)\n",
    "test_csv.to_csv(IMG_PATH + 'test_labels.csv')\n",
    "unique_classes = sorted(train_csv['class'].unique())\n",
    "\n",
    "# Print results\n",
    "print(\"Unique classes:\", unique_classes)\n",
    "print(\"\")\n",
    "print(\"Train images:\", train_csv['filename'].unique().shape[0])\n",
    "print(f\"Train annotations: {train_csv.shape[0]:,}\")\n",
    "print(\"Annotations per class (training):\")\n",
    "print(train_csv['class'].value_counts())\n",
    "print(\"\")\n",
    "print(\"Test images:\", test_csv['filename'].unique().shape[0])\n",
    "print(f\"Test annotations: {test_csv.shape[0]:,}\")\n",
    "print(\"\")\n",
    "\n",
    "# Generate label-id mapping\n",
    "class_mapping = []\n",
    "for count, item in enumerate(unique_classes):\n",
    "    class_mapping.append('item{')\n",
    "    class_mapping.append('\\tid: ' + str(count + 1))\n",
    "    class_mapping.append('\\tname: \\'' + item + '\\'')\n",
    "    class_mapping.append('}')\n",
    "with open(MAIN_PATH + 'training/labelmap.pbtxt', 'w') as f:\n",
    "    f.write('\\n'.join(class_mapping))\n",
    "print(\"Label id mapping:\")\n",
    "print('\\n'.join(class_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def class_text_to_int(row_label):\r\n",
      "    if row_label == 'cabinet':\r\n",
      "        return 1\r\n",
      "    elif row_label == 'chair':\r\n",
      "        return 2\r\n",
      "    elif row_label == 'countertop':\r\n",
      "        return 3\r\n",
      "    elif row_label == 'dishwasher':\r\n",
      "        return 4\r\n",
      "    elif row_label == 'outlet':\r\n",
      "        return 5\r\n",
      "    elif row_label == 'oven':\r\n",
      "        return 6\r\n",
      "    elif row_label == 'sofa':\r\n",
      "        return 7\r\n",
      "    elif row_label == 'stool':\r\n",
      "        return 8\r\n",
      "    elif row_label == 'stove':\r\n",
      "        return 9\r\n",
      "    elif row_label == 'utensil':\r\n",
      "        return 10 \r\n",
      "    else:\r\n",
      "        None\r\n"
     ]
    }
   ],
   "source": [
    "!head generate_tfrecord.py -n 53 | tail -n 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy `generate_tfrecord.py` to main folder and:\n",
    "1. replace `import tensorflow` with `import tensorflow.compat.v1`.\n",
    "2. Modify `class_test_to_int()` function with label-id mapping.\n",
    "3. Line 91: add `'image/object/difficult': dataset_util.int64_list_feature([0]*len(xmins)),`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:802: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 10. \n",
      "  warnings.warn(str(msg))\n",
      "Successfully created the TFRecords: /happy-walrus/models/version_3/train.record\n",
      "Successfully created the TFRecords: /happy-walrus/models/version_3/test.record\n",
      "\n",
      "-rw-r--r-- 1 root   root  60M Nov 27 14:12 test.record\n",
      "-rw-r--r-- 1 root   root 489M Nov 27 14:12 train.record\n"
     ]
    }
   ],
   "source": [
    "# Generate TF formatted data\n",
    "!python generate_tfrecord.py --csv_input={IMG_PATH}train_labels.csv --image_dir={IMG_PATH}train --output_path=train.record\n",
    "!python generate_tfrecord.py --csv_input={IMG_PATH}test_labels.csv --image_dir={IMG_PATH}test --output_path=test.record\n",
    "print(\"\")\n",
    "!ls -lh | grep .*\\.record$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get model checkpoint (other models in Tensorflow object detection's [Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md)). Models:\n",
    "- [faster rcnn inception v2 coco](http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_v2_coco_2018_01_28.tar.gz)\n",
    "    - Speed: 58\n",
    "    - mAP: 28\n",
    "- [faster rcnn resnet101 coco](http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet50_coco_2018_01_28.tar.gz)\n",
    "    - Speed: 106\n",
    "    - mAP: 32\n",
    "- [inception resnet v2 atrous](http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_resnet_v2_atrous_coco_2018_01_28.tar.gz)\n",
    "    - Speed: 620\n",
    "    - mAP: 37\n",
    "- [faster_rcnn_inception_resnet_v2_atrous_oid_v4](http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_resnet_v2_atrous_oid_v4_2018_12_12.tar.gz)\n",
    "    - Speed: 425\n",
    "    - mAP: 54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faster_rcnn_inception_v2_coco_2018_01_28/\n",
      "faster_rcnn_inception_v2_coco_2018_01_28/model.ckpt.index\n",
      "faster_rcnn_inception_v2_coco_2018_01_28/checkpoint\n",
      "faster_rcnn_inception_v2_coco_2018_01_28/pipeline.config\n",
      "faster_rcnn_inception_v2_coco_2018_01_28/model.ckpt.data-00000-of-00001\n",
      "faster_rcnn_inception_v2_coco_2018_01_28/model.ckpt.meta\n",
      "faster_rcnn_inception_v2_coco_2018_01_28/saved_model/\n",
      "faster_rcnn_inception_v2_coco_2018_01_28/saved_model/saved_model.pb\n",
      "faster_rcnn_inception_v2_coco_2018_01_28/saved_model/variables/\n",
      "faster_rcnn_inception_v2_coco_2018_01_28/frozen_inference_graph.pb\n"
     ]
    }
   ],
   "source": [
    "!wget \"http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_resnet_v2_atrous_coco_2018_01_28.tar.gz\"\n",
    "!tar xvzf faster_rcnn_inception_resnet_v2_atrous_coco_2018_01_28.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Copy config file from `./samples/configs/` to `./training/`\n",
    "2. Adapt parameters in config file:\n",
    "    - `num_classes`\n",
    "    - model `type`\n",
    "    - `fine_tune_checkpoint`\n",
    "    - train_input_reader: `input_path`, `label_map_path`\n",
    "    - `num_examples`: images/test/\n",
    "    - eval_input_reader: `input_path`, `label_map_path`\n",
    "    - `data_augmentation_options`, see augmentations and arguments in [preprocessor module](https://github.com/tensorflow/models/blob/master/research/object_detection/protos/preprocessor.proto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: this overwrites existing config file (copy only if no config already created)\n",
    "!cp {OD_PATH}samples/configs/faster_rcnn_inception_v2_pets.config {MAIN_PATH}training/faster_rcnn_inception_v2_kitchens.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move train.py from `legacy/` to `.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp {OD_PATH}legacy/train.py ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model\n",
    "- Make sure to delete all files in `training/` except `labelmap.pbtxt` and the config file.\n",
    "- Stops at 200.000 steps. Stop manually if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CONFIG_PATH = TRAIN_PATH + MODEL_NAME[:-1] + '.config'\n",
    "!python train.py \\\n",
    "    --logtostderr \\\n",
    "    --train_dir={TRAIN_PATH} \\\n",
    "    --pipeline_config_path={CONFIG_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In command line, inside container, run `tensorboard --logdir=/happy-walrus/models/version_2/training`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 130376\r\n",
      "-rw-r--r-- 1 root root      3726 Nov 25 21:14 autoaugm_inceptionv2.config\r\n",
      "-rw-r--r-- 1 root root       203 Nov 26 13:57 checkpoint\r\n",
      "-rw-r--r-- 1 root root  12079326 Nov 26 13:57 events.out.tfevents.1574776637.5df86dfc3c25\r\n",
      "-rw-r--r-- 1 root root  11785420 Nov 26 13:57 graph.pbtxt\r\n",
      "-rw-r--r-- 1 root root 103337376 Nov 26 13:57 model.ckpt-0.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 root root     25565 Nov 26 13:57 model.ckpt-0.index\r\n",
      "-rw-r--r-- 1 root root   6249782 Nov 26 13:57 model.ckpt-0.meta\r\n",
      "-rw-r--r-- 1 root root      3726 Nov 26 13:56 pipeline.config\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l {TRAIN_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /models/research/slim/nets/inception_resnet_v2.py:373: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /models/research/slim/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /models/research/object_detection/export_inference_graph.py:162: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
      "\n",
      "WARNING:tensorflow:From /models/research/object_detection/export_inference_graph.py:145: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "W1126 14:02:04.583538 140266282092352 module_wrapper.py:139] From /models/research/object_detection/export_inference_graph.py:145: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From /models/research/object_detection/exporter.py:389: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n",
      "W1126 14:02:04.591241 140266282092352 module_wrapper.py:139] From /models/research/object_detection/exporter.py:389: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n",
      "WARNING:tensorflow:From /models/research/object_detection/exporter.py:121: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1126 14:02:04.591506 140266282092352 module_wrapper.py:139] From /models/research/object_detection/exporter.py:121: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /models/research/object_detection/core/preprocessor.py:2689: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
      "\n",
      "W1126 14:02:04.643486 140266282092352 module_wrapper.py:139] From /models/research/object_detection/core/preprocessor.py:2689: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
      "\n",
      "WARNING:tensorflow:From /models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:166: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W1126 14:02:04.707534 140266282092352 module_wrapper.py:139] From /models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:166: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:2784: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "W1126 14:02:04.720016 140266282092352 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:2784: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /models/research/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
      "\n",
      "W1126 14:02:06.675214 140266282092352 module_wrapper.py:139] From /models/research/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
      "\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "I1126 14:02:06.684060 140266282092352 regularizers.py:98] Scale of 0 disables regularizer.\n",
      "WARNING:tensorflow:From /models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:556: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
      "\n",
      "W1126 14:02:06.684453 140266282092352 module_wrapper.py:139] From /models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:556: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
      "\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "I1126 14:02:06.703278 140266282092352 regularizers.py:98] Scale of 0 disables regularizer.\n",
      "WARNING:tensorflow:From /models/research/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "W1126 14:02:06.703807 140266282092352 module_wrapper.py:139] From /models/research/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I1126 14:02:06.703952 140266282092352 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
      "WARNING:tensorflow:From /models/research/object_detection/core/box_list_ops.py:141: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1126 14:02:06.774621 140266282092352 deprecation.py:323] From /models/research/object_detection/core/box_list_ops.py:141: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /models/research/object_detection/utils/spatial_transform_ops.py:419: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n",
      "W1126 14:02:07.495993 140266282092352 deprecation.py:506] From /models/research/object_detection/utils/spatial_transform_ops.py:419: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n",
      "WARNING:tensorflow:From /models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:189: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "W1126 14:02:07.519796 140266282092352 module_wrapper.py:139] From /models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:189: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "W1126 14:02:08.169629 140266282092352 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "I1126 14:02:08.177183 140266282092352 regularizers.py:98] Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "I1126 14:02:08.204251 140266282092352 regularizers.py:98] Scale of 0 disables regularizer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/dispatch.py:180: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n",
      "Instructions for updating:\n",
      "`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n",
      "W1126 14:02:09.366701 140266282092352 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/dispatch.py:180: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n",
      "Instructions for updating:\n",
      "`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /models/research/object_detection/exporter.py:268: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "W1126 14:02:09.914073 140266282092352 module_wrapper.py:139] From /models/research/object_detection/exporter.py:268: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /models/research/object_detection/exporter.py:370: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_or_create_global_step\n",
      "W1126 14:02:09.914438 140266282092352 deprecation.py:323] From /models/research/object_detection/exporter.py:370: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_or_create_global_step\n",
      "WARNING:tensorflow:From /models/research/object_detection/exporter.py:402: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1126 14:02:09.918906 140266282092352 module_wrapper.py:139] From /models/research/object_detection/exporter.py:402: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /models/research/object_detection/exporter.py:526: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n",
      "Instructions for updating:\n",
      "Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n",
      "W1126 14:02:09.919153 140266282092352 deprecation.py:323] From /models/research/object_detection/exporter.py:526: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n",
      "Instructions for updating:\n",
      "Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
      "W1126 14:02:09.920657 140266282092352 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
      "250 ops no flops stats due to incomplete shapes.\n",
      "Parsing Inputs...\n",
      "Incomplete shape.\n",
      "\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              0\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   name\n",
      "-account_type_regexes       _trainable_variables\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          .*BatchNorm.*\n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     params\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "Incomplete shape.\n",
      "\n",
      "Doc:\n",
      "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
      "param: Number of parameters (in the Variable).\n",
      "\n",
      "Profile:\n",
      "node name | # parameters\n",
      "_TFProfRoot (--/12.89m params)\n",
      "  Conv (--/2.65m params)\n",
      "    Conv/biases (512, 512/512 params)\n",
      "    Conv/weights (3x3x576x512, 2.65m/2.65m params)\n",
      "  FirstStageBoxPredictor (--/36.94k params)\n",
      "    FirstStageBoxPredictor/BoxEncodingPredictor (--/24.62k params)\n",
      "      FirstStageBoxPredictor/BoxEncodingPredictor/biases (48, 48/48 params)\n",
      "      FirstStageBoxPredictor/BoxEncodingPredictor/weights (1x1x512x48, 24.58k/24.58k params)\n",
      "    FirstStageBoxPredictor/ClassPredictor (--/12.31k params)\n",
      "      FirstStageBoxPredictor/ClassPredictor/biases (24, 24/24 params)\n",
      "      FirstStageBoxPredictor/ClassPredictor/weights (1x1x512x24, 12.29k/12.29k params)\n",
      "  FirstStageFeatureExtractor (--/4.25m params)\n",
      "    FirstStageFeatureExtractor/InceptionV2 (--/4.25m params)\n",
      "      FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7 (--/2.71k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm (--/0 params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/depthwise_weights (7x7x3x8, 1.18k/1.18k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/pointwise_weights (1x1x24x64, 1.54k/1.54k params)\n",
      "      FirstStageFeatureExtractor/InceptionV2/Conv2d_2b_1x1 (--/4.10k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Conv2d_2b_1x1/BatchNorm (--/0 params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Conv2d_2b_1x1/weights (1x1x64x64, 4.10k/4.10k params)\n",
      "      FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3 (--/110.59k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3/BatchNorm (--/0 params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3/weights (3x3x64x192, 110.59k/110.59k params)\n",
      "      FirstStageFeatureExtractor/InceptionV2/Mixed_3b (--/218.11k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_0 (--/12.29k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1 (--/12.29k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/weights (1x1x192x64, 12.29k/12.29k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1 (--/49.15k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1 (--/12.29k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/weights (1x1x192x64, 12.29k/12.29k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3 (--/36.86k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/weights (3x3x64x64, 36.86k/36.86k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2 (--/150.53k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1 (--/12.29k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/weights (1x1x192x64, 12.29k/12.29k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3 (--/55.30k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/weights (3x3x64x96, 55.30k/55.30k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3 (--/82.94k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/weights (3x3x96x96, 82.94k/82.94k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_3 (--/6.14k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1 (--/6.14k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/weights (1x1x192x32, 6.14k/6.14k params)\n",
      "      FirstStageFeatureExtractor/InceptionV2/Mixed_3c (--/259.07k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_0 (--/16.38k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1 (--/16.38k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/weights (1x1x256x64, 16.38k/16.38k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1 (--/71.68k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1 (--/16.38k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/weights (1x1x256x64, 16.38k/16.38k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3 (--/55.30k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/weights (3x3x64x96, 55.30k/55.30k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2 (--/154.62k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1 (--/16.38k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/weights (1x1x256x64, 16.38k/16.38k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3 (--/55.30k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/weights (3x3x64x96, 55.30k/55.30k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3 (--/82.94k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/weights (3x3x96x96, 82.94k/82.94k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_3 (--/16.38k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1 (--/16.38k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/weights (1x1x256x64, 16.38k/16.38k params)\n",
      "      FirstStageFeatureExtractor/InceptionV2/Mixed_4a (--/384.00k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0 (--/225.28k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1 (--/40.96k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/weights (1x1x320x128, 40.96k/40.96k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3 (--/184.32k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/weights (3x3x128x160, 184.32k/184.32k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1 (--/158.72k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1 (--/20.48k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/weights (1x1x320x64, 20.48k/20.48k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3 (--/55.30k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/weights (3x3x64x96, 55.30k/55.30k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3 (--/82.94k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/weights (3x3x96x96, 82.94k/82.94k params)\n",
      "      FirstStageFeatureExtractor/InceptionV2/Mixed_4b (--/608.26k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_0 (--/129.02k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1 (--/129.02k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/weights (1x1x576x224, 129.02k/129.02k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1 (--/92.16k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1 (--/36.86k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/weights (1x1x576x64, 36.86k/36.86k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3 (--/55.30k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/weights (3x3x64x96, 55.30k/55.30k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2 (--/313.34k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1 (--/55.30k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3 (--/110.59k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/weights (3x3x96x128, 110.59k/110.59k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3 (--/147.46k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/weights (3x3x128x128, 147.46k/147.46k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_3 (--/73.73k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1 (--/73.73k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n",
      "      FirstStageFeatureExtractor/InceptionV2/Mixed_4c (--/663.55k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_0 (--/110.59k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1 (--/110.59k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/weights (1x1x576x192, 110.59k/110.59k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1 (--/165.89k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1 (--/55.30k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3 (--/110.59k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/weights (3x3x96x128, 110.59k/110.59k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2 (--/313.34k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1 (--/55.30k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3 (--/110.59k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/weights (3x3x96x128, 110.59k/110.59k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3 (--/147.46k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/weights (3x3x128x128, 147.46k/147.46k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_3 (--/73.73k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1 (--/73.73k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n",
      "      FirstStageFeatureExtractor/InceptionV2/Mixed_4d (--/893.95k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_0 (--/92.16k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1 (--/92.16k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/weights (1x1x576x160, 92.16k/92.16k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1 (--/258.05k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1 (--/73.73k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3 (--/184.32k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/weights (3x3x128x160, 184.32k/184.32k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2 (--/488.45k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1 (--/73.73k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3 (--/184.32k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/weights (3x3x128x160, 184.32k/184.32k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3 (--/230.40k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/weights (3x3x160x160, 230.40k/230.40k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_3 (--/55.30k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1 (--/55.30k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n",
      "      FirstStageFeatureExtractor/InceptionV2/Mixed_4e (--/1.11m params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_0 (--/55.30k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1 (--/55.30k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1 (--/294.91k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1 (--/73.73k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3 (--/221.18k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/weights (3x3x128x192, 221.18k/221.18k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2 (--/700.42k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1 (--/92.16k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/weights (1x1x576x160, 92.16k/92.16k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3 (--/276.48k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/weights (3x3x160x192, 276.48k/276.48k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3 (--/331.78k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/weights (3x3x192x192, 331.78k/331.78k params)\n",
      "        FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_3 (--/55.30k params)\n",
      "          FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1 (--/55.30k params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n",
      "            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n",
      "  SecondStageBoxPredictor (--/52.27k params)\n",
      "    SecondStageBoxPredictor/BoxEncodingPredictor (--/41.00k params)\n",
      "      SecondStageBoxPredictor/BoxEncodingPredictor/biases (40, 40/40 params)\n",
      "      SecondStageBoxPredictor/BoxEncodingPredictor/weights (1024x40, 40.96k/40.96k params)\n",
      "    SecondStageBoxPredictor/ClassPredictor (--/11.28k params)\n",
      "      SecondStageBoxPredictor/ClassPredictor/biases (11, 11/11 params)\n",
      "      SecondStageBoxPredictor/ClassPredictor/weights (1024x11, 11.26k/11.26k params)\n",
      "  SecondStageFeatureExtractor (--/5.89m params)\n",
      "    SecondStageFeatureExtractor/InceptionV2 (--/5.89m params)\n",
      "      SecondStageFeatureExtractor/InceptionV2/Mixed_5a (--/1.44m params)\n",
      "        SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0 (--/294.91k params)\n",
      "          SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1 (--/73.73k params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n",
      "          SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3 (--/221.18k params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/BatchNorm (--/0 params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/weights (3x3x128x192, 221.18k/221.18k params)\n",
      "        SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1 (--/1.14m params)\n",
      "          SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1 (--/110.59k params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/weights (1x1x576x192, 110.59k/110.59k params)\n",
      "          SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3 (--/442.37k params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/weights (3x3x192x256, 442.37k/442.37k params)\n",
      "          SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3 (--/589.82k params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/BatchNorm (--/0 params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/weights (3x3x256x256, 589.82k/589.82k params)\n",
      "      SecondStageFeatureExtractor/InceptionV2/Mixed_5b (--/2.18m params)\n",
      "        SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_0 (--/360.45k params)\n",
      "          SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1 (--/360.45k params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights (1x1x1024x352, 360.45k/360.45k params)\n",
      "        SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1 (--/749.57k params)\n",
      "          SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1 (--/196.61k params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights (1x1x1024x192, 196.61k/196.61k params)\n",
      "          SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3 (--/552.96k params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/weights (3x3x192x320, 552.96k/552.96k params)\n",
      "        SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2 (--/937.98k params)\n",
      "          SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1 (--/163.84k params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights (1x1x1024x160, 163.84k/163.84k params)\n",
      "          SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3 (--/322.56k params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights (3x3x160x224, 322.56k/322.56k params)\n",
      "          SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3 (--/451.58k params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights (3x3x224x224, 451.58k/451.58k params)\n",
      "        SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_3 (--/131.07k params)\n",
      "          SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1 (--/131.07k params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights (1x1x1024x128, 131.07k/131.07k params)\n",
      "      SecondStageFeatureExtractor/InceptionV2/Mixed_5c (--/2.28m params)\n",
      "        SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_0 (--/360.45k params)\n",
      "          SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1 (--/360.45k params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights (1x1x1024x352, 360.45k/360.45k params)\n",
      "        SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1 (--/749.57k params)\n",
      "          SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1 (--/196.61k params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/weights (1x1x1024x192, 196.61k/196.61k params)\n",
      "          SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3 (--/552.96k params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/weights (3x3x192x320, 552.96k/552.96k params)\n",
      "        SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2 (--/1.04m params)\n",
      "          SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1 (--/196.61k params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights (1x1x1024x192, 196.61k/196.61k params)\n",
      "          SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3 (--/387.07k params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights (3x3x192x224, 387.07k/387.07k params)\n",
      "          SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3 (--/451.58k params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights (3x3x224x224, 451.58k/451.58k params)\n",
      "        SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_3 (--/131.07k params)\n",
      "          SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1 (--/131.07k params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n",
      "            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights (1x1x1024x128, 131.07k/131.07k params)\n",
      "\n",
      "======================End of Report==========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 ops no flops stats due to incomplete shapes.\n",
      "Parsing Inputs...\n",
      "Incomplete shape.\n",
      "\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              1\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   float_ops\n",
      "-account_type_regexes       .*\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          .*BatchNorm.*,.*Initializer.*,.*Regularizer.*,.*BiasAdd.*\n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     float_ops\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "Incomplete shape.\n",
      "\n",
      "Doc:\n",
      "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
      "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
      "\n",
      "Profile:\n",
      "node name | # float_ops\n",
      "_TFProfRoot (--/6.19k flops)\n",
      "  SecondStagePostprocessor/map/while/ClipToWindow/Maximum_3 (300/300 flops)\n",
      "  SecondStagePostprocessor/map/while/ToNormalizedCoordinates/Scale/mul_3 (300/300 flops)\n",
      "  SecondStagePostprocessor/map/while/ClipToWindow/Minimum_3 (300/300 flops)\n",
      "  SecondStagePostprocessor/map/while/ToNormalizedCoordinates/Scale/mul_2 (300/300 flops)\n",
      "  map/while/ToNormalizedCoordinates/Scale/mul (300/300 flops)\n",
      "  map/while/ToNormalizedCoordinates/Scale/mul_1 (300/300 flops)\n",
      "  map/while/ToNormalizedCoordinates/Scale/mul_2 (300/300 flops)\n",
      "  map/while/ToNormalizedCoordinates/Scale/mul_3 (300/300 flops)\n",
      "  SecondStagePostprocessor/map/while/ClipToWindow/Minimum_2 (300/300 flops)\n",
      "  SecondStagePostprocessor/map/while/ClipToWindow/Minimum_1 (300/300 flops)\n",
      "  SecondStagePostprocessor/map/while/ClipToWindow/Minimum (300/300 flops)\n",
      "  SecondStagePostprocessor/map/while/ClipToWindow/Maximum_2 (300/300 flops)\n",
      "  SecondStagePostprocessor/map/while/ClipToWindow/Maximum_1 (300/300 flops)\n",
      "  SecondStagePostprocessor/map/while/ClipToWindow/Maximum (300/300 flops)\n",
      "  SecondStagePostprocessor/map/while/ToNormalizedCoordinates/Scale/mul_1 (300/300 flops)\n",
      "  map_2/while/mul (300/300 flops)\n",
      "  map_2/while/mul_1 (300/300 flops)\n",
      "  map_2/while/mul_2 (300/300 flops)\n",
      "  map_2/while/mul_3 (300/300 flops)\n",
      "  SecondStagePostprocessor/map/while/ToNormalizedCoordinates/Scale/mul (300/300 flops)\n",
      "  GridAnchorGenerator/mul_1 (12/12 flops)\n",
      "  GridAnchorGenerator/mul_2 (12/12 flops)\n",
      "  GridAnchorGenerator/mul (12/12 flops)\n",
      "  GridAnchorGenerator/truediv (12/12 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_2 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_1 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_4 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_3 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_8 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_5 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_6 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_7 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_8 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_9 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_1 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_10 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_11 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_2 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_12 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_13 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_14 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_15 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_16 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_17 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_18 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_19 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_3 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_4 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_5 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_6 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_7 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_5 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_9 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_9 (1/1 flops)\n",
      "  mul (1/1 flops)\n",
      "  map_2/while/Less_1 (1/1 flops)\n",
      "  map_2/while/Less (1/1 flops)\n",
      "  map_1/while/ToNormalizedCoordinates/truediv_1 (1/1 flops)\n",
      "  map_1/while/ToNormalizedCoordinates/truediv (1/1 flops)\n",
      "  map_1/while/Less_1 (1/1 flops)\n",
      "  map_1/while/Less (1/1 flops)\n",
      "  map/while/ToNormalizedCoordinates/truediv_1 (1/1 flops)\n",
      "  map/while/ToNormalizedCoordinates/truediv (1/1 flops)\n",
      "  map/while/Less_1 (1/1 flops)\n",
      "  map/while/Less (1/1 flops)\n",
      "  SecondStagePostprocessor/map/while/ToNormalizedCoordinates/truediv_1 (1/1 flops)\n",
      "  SecondStagePostprocessor/map/while/ToNormalizedCoordinates/truediv (1/1 flops)\n",
      "  SecondStagePostprocessor/map/while/Less_1 (1/1 flops)\n",
      "  SecondStagePostprocessor/map/while/Less (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_12 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_8 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_7 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_6 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_1 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_4 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_3 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_2 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_19 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_18 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_17 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_16 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_15 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_14 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_13 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_5 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_11 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_12 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_13 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_14 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_15 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_16 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_17 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_2 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_3 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_4 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_10 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_6 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_7 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_8 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_9 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/ones/Less (1/1 flops)\n",
      "  FirstStageFeatureExtractor/GreaterEqual (1/1 flops)\n",
      "  FirstStageFeatureExtractor/GreaterEqual_1 (1/1 flops)\n",
      "  GridAnchorGenerator/assert_equal_1/Equal (1/1 flops)\n",
      "  GridAnchorGenerator/mul_7 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_1 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/Less_1 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Greater (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_1 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField_1/Equal (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_1 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater (1/1 flops)\n",
      "  GridAnchorGenerator/mul_8 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_2 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_3 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_4 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_5 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_6 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_7 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_8 (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_1 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_5 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/sub_1 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/truediv (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/truediv_1 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Greater (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_1 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_10 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_2 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_3 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_4 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/sub (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_6 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_7 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_8 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_9 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField_1/Equal (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub (1/1 flops)\n",
      "  BatchMultiClassNonMaxSuppression/map/while/Less (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_10 (1/1 flops)\n",
      "  Preprocessor/map/while/ResizeToRange/cond/resize_1/Minimum (1/1 flops)\n",
      "  GridAnchorGenerator/zeros/Less (1/1 flops)\n",
      "  Preprocessor/map/while/Less (1/1 flops)\n",
      "  Preprocessor/map/while/Less_1 (1/1 flops)\n",
      "  Preprocessor/map/while/ResizeToRange/Less (1/1 flops)\n",
      "  Preprocessor/map/while/ResizeToRange/cond/resize/Minimum (1/1 flops)\n",
      "  Preprocessor/map/while/ResizeToRange/cond/resize/mul (1/1 flops)\n",
      "  Preprocessor/map/while/ResizeToRange/cond/resize/mul_1 (1/1 flops)\n",
      "  Preprocessor/map/while/ResizeToRange/cond/resize/truediv (1/1 flops)\n",
      "  Preprocessor/map/while/ResizeToRange/cond/resize/truediv_1 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_11 (1/1 flops)\n",
      "  Preprocessor/map/while/ResizeToRange/cond/resize_1/mul (1/1 flops)\n",
      "  Preprocessor/map/while/ResizeToRange/cond/resize_1/mul_1 (1/1 flops)\n",
      "  Preprocessor/map/while/ResizeToRange/cond/resize_1/truediv (1/1 flops)\n",
      "  Preprocessor/map/while/ResizeToRange/cond/resize_1/truediv_1 (1/1 flops)\n",
      "  SecondStageDetectionFeaturesExtract/mul (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchGather/mul (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchGather/mul_2 (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/Less (1/1 flops)\n",
      "  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/Less_1 (1/1 flops)\n",
      "\n",
      "======================End of Report==========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /models/research/object_detection/exporter.py:419: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "W1126 14:02:12.294680 140266282092352 module_wrapper.py:139] From /models/research/object_detection/exporter.py:419: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From /models/research/object_detection/exporter.py:332: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W1126 14:02:13.548985 140266282092352 module_wrapper.py:139] From /models/research/object_detection/exporter.py:332: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2019-11-26 14:02:13.549601: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2019-11-26 14:02:13.549646: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2019-11-26 14:02:13.549684: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (5df86dfc3c25): /proc/driver/nvidia/version does not exist\n",
      "2019-11-26 14:02:13.550022: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-11-26 14:02:13.560333: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100060000 Hz\n",
      "2019-11-26 14:02:13.561095: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x9b44580 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2019-11-26 14:02:13.561136: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "INFO:tensorflow:Restoring parameters from /happy-walrus/models/version_3/training/autoaugm_inceptionv2/model.ckpt-0\n",
      "I1126 14:02:13.565539 140266282092352 saver.py:1284] Restoring parameters from /happy-walrus/models/version_3/training/autoaugm_inceptionv2/model.ckpt-0\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "W1126 14:02:15.598072 140266282092352 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /happy-walrus/models/version_3/training/autoaugm_inceptionv2/model.ckpt-0\n",
      "I1126 14:02:16.526192 140266282092352 saver.py:1284] Restoring parameters from /happy-walrus/models/version_3/training/autoaugm_inceptionv2/model.ckpt-0\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "W1126 14:02:17.629819 140266282092352 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "W1126 14:02:17.630131 140266282092352 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "INFO:tensorflow:Froze 356 variables.\n",
      "I1126 14:02:18.298142 140266282092352 graph_util_impl.py:334] Froze 356 variables.\n",
      "INFO:tensorflow:Converted 356 variables to const ops.\n",
      "I1126 14:02:18.468168 140266282092352 graph_util_impl.py:394] Converted 356 variables to const ops.\n",
      "WARNING:tensorflow:From /models/research/object_detection/exporter.py:296: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.\n",
      "\n",
      "W1126 14:02:19.697732 140266282092352 module_wrapper.py:139] From /models/research/object_detection/exporter.py:296: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.\n",
      "\n",
      "WARNING:tensorflow:From /models/research/object_detection/exporter.py:299: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "W1126 14:02:19.698277 140266282092352 deprecation.py:323] From /models/research/object_detection/exporter.py:299: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "WARNING:tensorflow:From /models/research/object_detection/exporter.py:305: The name tf.saved_model.signature_def_utils.build_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.build_signature_def instead.\n",
      "\n",
      "W1126 14:02:19.698874 140266282092352 module_wrapper.py:139] From /models/research/object_detection/exporter.py:305: The name tf.saved_model.signature_def_utils.build_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.build_signature_def instead.\n",
      "\n",
      "WARNING:tensorflow:From /models/research/object_detection/exporter.py:308: The name tf.saved_model.signature_constants.PREDICT_METHOD_NAME is deprecated. Please use tf.saved_model.PREDICT_METHOD_NAME instead.\n",
      "\n",
      "W1126 14:02:19.699087 140266282092352 module_wrapper.py:139] From /models/research/object_detection/exporter.py:308: The name tf.saved_model.signature_constants.PREDICT_METHOD_NAME is deprecated. Please use tf.saved_model.PREDICT_METHOD_NAME instead.\n",
      "\n",
      "WARNING:tensorflow:From /models/research/object_detection/exporter.py:313: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
      "\n",
      "W1126 14:02:19.699385 140266282092352 module_wrapper.py:139] From /models/research/object_detection/exporter.py:313: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
      "\n",
      "WARNING:tensorflow:From /models/research/object_detection/exporter.py:315: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.\n",
      "\n",
      "W1126 14:02:19.699562 140266282092352 module_wrapper.py:139] From /models/research/object_detection/exporter.py:315: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.\n",
      "\n",
      "INFO:tensorflow:No assets to save.\n",
      "I1126 14:02:19.699909 140266282092352 builder_impl.py:640] No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "I1126 14:02:19.700032 140266282092352 builder_impl.py:460] No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /happy-walrus/models/version_3/inference_graph/autoaugm_inceptionv2/saved_model/saved_model.pb\n",
      "I1126 14:02:20.411642 140266282092352 builder_impl.py:425] SavedModel written to: /happy-walrus/models/version_3/inference_graph/autoaugm_inceptionv2/saved_model/saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /models/research/object_detection/utils/config_util.py:188: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\r\n",
      "\r\n",
      "W1126 14:02:20.461877 140266282092352 module_wrapper.py:139] From /models/research/object_detection/utils/config_util.py:188: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\r\n",
      "\r\n",
      "INFO:tensorflow:Writing pipeline config file to /happy-walrus/models/version_3/inference_graph/autoaugm_inceptionv2/pipeline.config\r\n",
      "I1126 14:02:20.462159 140266282092352 config_util.py:190] Writing pipeline config file to /happy-walrus/models/version_3/inference_graph/autoaugm_inceptionv2/pipeline.config\r\n"
     ]
    }
   ],
   "source": [
    "ckpt_step=0\n",
    "!python {OD_PATH}export_inference_graph.py \\\n",
    "    --input_type image_tensor \\\n",
    "    --pipeline_config_path {CONFIG_PATH} \\\n",
    "    --trained_checkpoint_prefix {TRAIN_PATH}model.ckpt-{ckpt_step} \\\n",
    "    --output_directory {GRAPH_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 102M\r\n",
      "-rw-r--r-- 1 root root   77 Nov 26 14:02 checkpoint\r\n",
      "-rw-r--r-- 1 root root  51M Nov 26 14:02 frozen_inference_graph.pb\r\n",
      "-rw-r--r-- 1 root root  50M Nov 26 14:02 model.ckpt.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 root root  16K Nov 26 14:02 model.ckpt.index\r\n",
      "-rw-r--r-- 1 root root 2.0M Nov 26 14:02 model.ckpt.meta\r\n",
      "-rw-r--r-- 1 root root 3.2K Nov 26 14:02 pipeline.config\r\n",
      "drwxr-xr-x 3 root root 4.0K Nov 26 14:02 saved_model\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {GRAPH_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download model and store it in Google Drive for use in next section (not necessary if model is already in disk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_name = GRAPH_PATH + MODEL_NAME[:-1] + '.zip'\n",
    "!zip -qr {zip_name} {GRAPH_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download model (only if model is not on disk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T12:06:30.407521Z",
     "start_time": "2019-11-07T12:06:29.516282Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1JVFhXqaayxWOIhuQdzmjIMBAHrAhLcEQ\n",
      "To: /happy-walrus/models/version_2/inference_graph.zip\n",
      "144MB [00:01, 74.9MB/s] \n"
     ]
    }
   ],
   "source": [
    "# Download model\n",
    "model_id = \"1JVFhXqaayxWOIhuQdzmjIMBAHrAhLcEQ\"\n",
    "model_name = \"inference_graph.zip\"\n",
    "gdown.download(url=\"https://drive.google.com/uc?id=\" + model_id,\n",
    "               output=MAIN_PATH + model_name,\n",
    "               quiet=False)\n",
    "\n",
    "# Unzip model\n",
    "!unzip -q {MAIN_PATH}{model_name} -d {MAIN_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To `Object_detection_image.py`:\n",
    "1. add before last line `cv2.imwrite('/path/to/save/boxed_image.JPG', image)` and comment last line out.\n",
    "1. `NUM_CLASSES`\n",
    "1. `IMAGE_NAME`\n",
    "1. Replace `from utils` with `from object_detection.utils`\n",
    "1. Modify so it draws boxes in all images in `images/test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Image Object Detection Using Tensorflow-trained Classifier #########\r\n",
      "#\r\n",
      "# Author: Evan Juras\r\n",
      "# Date: 1/15/18\r\n",
      "# Description: \r\n",
      "# This program uses a TensorFlow-trained neural network to perform object detection.\r\n",
      "# It loads the classifier and uses it to perform object detection on an image.\r\n",
      "# It draws boxes, scores, and labels around the objects of interest in the image.\r\n",
      "\r\n",
      "## Some of the code is copied from Google's example at\r\n",
      "## https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\r\n",
      "\r\n",
      "## and some is copied from Dat Tran's example at\r\n",
      "## https://github.com/datitran/object_detector_app/blob/master/object_detection_app.py\r\n",
      "\r\n",
      "## but I changed it to make it more understandable to me.\r\n",
      "\r\n",
      "# Import packages\r\n",
      "import os\r\n",
      "import cv2\r\n",
      "import numpy as np\r\n",
      "import tensorflow as tf\r\n",
      "import sys\r\n",
      "import re\r\n",
      "\r\n",
      "# This is needed since the notebook is stored in the object_detection folder.\r\n",
      "sys.path.append(\"..\")\r\n",
      "\r\n",
      "# Import utilites\r\n",
      "from object_detection.utils import label_map_util\r\n",
      "from object_detection.utils import visualization_utils as vis_util\r\n",
      "\r\n",
      "# Name of the directory containing the object detection module we're using\r\n",
      "#MODEL_NAME = 'inference_graph'\r\n",
      "#IMAGE = 'ADE_train_00000615.jpg'\r\n",
      "#IMAGE_NAME = 'images/test/' + IMAGE\r\n",
      "\r\n",
      "# Grab path to current working directory\r\n",
      "CWD_PATH = os.getcwd()\r\n",
      "\r\n",
      "# Path to frozen detection graph .pb file, which contains the model that is used\r\n",
      "# for object detection.\r\n",
      "PATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')\r\n",
      "\r\n",
      "# Path to label map file\r\n",
      "PATH_TO_LABELS = os.path.join(CWD_PATH,'training','labelmap.pbtxt')\r\n",
      "\r\n",
      "# Path to image\r\n",
      "PATH_TO_IMAGE = os.path.join(CWD_PATH,IMAGE_NAME)\r\n",
      "\r\n",
      "# Number of classes the object detector can identify\r\n",
      "NUM_CLASSES = 10 \r\n",
      "\r\n",
      "# Load the label map.\r\n",
      "# Label maps map indices to category names, so that when our convolution\r\n",
      "# network predicts `5`, we know that this corresponds to `king`.\r\n",
      "# Here we use internal utility functions, but anything that returns a\r\n",
      "# dictionary mapping integers to appropriate string labels would be fine\r\n",
      "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\r\n",
      "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\r\n",
      "category_index = label_map_util.create_category_index(categories)\r\n",
      "\r\n",
      "# Load the Tensorflow model into memory.\r\n",
      "detection_graph = tf.Graph()\r\n",
      "with detection_graph.as_default():\r\n",
      "    od_graph_def = tf.GraphDef()\r\n",
      "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\r\n",
      "        serialized_graph = fid.read()\r\n",
      "        od_graph_def.ParseFromString(serialized_graph)\r\n",
      "        tf.import_graph_def(od_graph_def, name='')\r\n",
      "\r\n",
      "    sess = tf.Session(graph=detection_graph)\r\n",
      "\r\n",
      "# Define input and output tensors (i.e. data) for the object detection classifier\r\n",
      "\r\n",
      "# Input tensor is the image\r\n",
      "image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\r\n",
      "\r\n",
      "# Output tensors are the detection boxes, scores, and classes\r\n",
      "# Each box represents a part of the image where a particular object was detected\r\n",
      "detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\r\n",
      "\r\n",
      "# Each score represents level of confidence for each of the objects.\r\n",
      "# The score is shown on the result image, together with the class label.\r\n",
      "detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\r\n",
      "detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\r\n",
      "\r\n",
      "# Number of objects detected\r\n",
      "num_detections = detection_graph.get_tensor_by_name('num_detections:0')\r\n",
      "\r\n",
      "test_image_path = '/happy-walrus/models/version_3/images/test/'\r\n",
      "list_images = [file for file in os.listdir(test_image_path) if re.search('\\.jpg', file)]\r\n",
      "for image_name in list_images:\r\n",
      "    # Load image using OpenCV and\r\n",
      "    # expand image dimensions to have shape: [1, None, None, 3]\r\n",
      "    # i.e. a single-column array, where each item in the column has the pixel RGB value\r\n",
      "    image = cv2.imread(test_image_path + image_name)\r\n",
      "    image_expanded = np.expand_dims(image, axis=0)\r\n",
      "    print(test_image_path + image_name)\r\n",
      "\r\n",
      "    # Perform the actual detection by running the model with the image as input\r\n",
      "    (boxes, scores, classes, num) = sess.run(\r\n",
      "        [detection_boxes, detection_scores, detection_classes, num_detections],\r\n",
      "        feed_dict={image_tensor: image_expanded})\r\n",
      "\r\n",
      "    # Draw the results of the detection (aka 'visulaize the results')\r\n",
      "    vis_util.visualize_boxes_and_labels_on_image_array(\r\n",
      "        image,\r\n",
      "        np.squeeze(boxes),\r\n",
      "        np.squeeze(classes).astype(np.int32),\r\n",
      "        np.squeeze(scores),\r\n",
      "        category_index,\r\n",
      "        use_normalized_coordinates=True,\r\n",
      "        line_thickness=8,\r\n",
      "        min_score_thresh=0.60)\r\n",
      "\r\n",
      "    cv2.imwrite('/happy-walrus/models/version_3/evaluation/auto_augmentation/bboxes/' + image_name[:-4] + '_bbox.jpg', image)\r\n",
      "\r\n",
      "# All the results have been drawn on image. Now display the image.\r\n",
      "#print(\"Write successful to \" + IMAGE[:-4] + '_pred.jpg')\r\n",
      "#cv2.imshow('Object detector', image)\r\n",
      "\r\n",
      "# Press any key to close the image\r\n",
      "# cv2.waitKey(0)\r\n",
      "\r\n",
      "# Clean up\r\n",
      "# cv2.destroyAllWindows()\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir {EVAL_PATH}\n",
    "!mkdir {EVAL_PATH}bboxes\n",
    "!cat {MAIN_PATH}Object_detection_image.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference:\n",
    "- If `NoneType` related error, reinstall opencv with `apt install opencv-python`.\n",
    "- Change save path to model specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /models/research/object_detection/utils/label_map_util.py:137: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From Object_detection_image.py:66: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\n",
      "\n",
      "WARNING:tensorflow:From Object_detection_image.py:72: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2019-11-15 15:32:20.262223: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2019-11-15 15:32:20.262274: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2019-11-15 15:32:20.262312: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (34c587f1c563): /proc/driver/nvidia/version does not exist\n",
      "2019-11-15 15:32:20.262643: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-11-15 15:32:20.271954: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100060000 Hz\n",
      "2019-11-15 15:32:20.273008: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7eb06e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2019-11-15 15:32:20.273100: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "/happy-walrus/models/version_2/images/test/grey_kitchen.NM.045.jpg\n",
      "/happy-walrus/models/version_2/images/test/European_kitchen.NM.024.jpg\n",
      "/happy-walrus/models/version_2/images/test/purple_kitchen.EH.017.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010176.jpg\n",
      "/happy-walrus/models/version_2/images/test/blue_kitchen.NM.069.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010390.jpg\n",
      "/happy-walrus/models/version_2/images/test/big_kitchen.NM.032.jpg\n",
      "/happy-walrus/models/version_2/images/test/large_kitchen.EH.006.jpg\n",
      "/happy-walrus/models/version_2/images/test/Indian_kitchen.NM.035.jpg\n",
      "/happy-walrus/models/version_2/images/test/Indian_kitchen.NM.042.jpg\n",
      "/happy-walrus/models/version_2/images/test/hot_stove.NM.010.jpg\n",
      "/happy-walrus/models/version_2/images/test/clean_kitchen.NM.020.jpg\n",
      "/happy-walrus/models/version_2/images/test/pintrest_clean_kitchen.EH.051.jpg\n",
      "/happy-walrus/models/version_2/images/test/pintrest_messy_kitchen.EH.005.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010587.jpg\n",
      "/happy-walrus/models/version_2/images/test/boiling_water.NM.009.jpg\n",
      "/happy-walrus/models/version_2/images/test/white_kitchen.EH.045.jpg\n",
      "/happy-walrus/models/version_2/images/test/old_kitchen.EH.046.jpg\n",
      "/happy-walrus/models/version_2/images/test/grey_kitchen.NM.005.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010178.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010349.jpg\n",
      "/happy-walrus/models/version_2/images/test/pintrest_cluttered_kitchen.EH.002.jpg\n",
      "/happy-walrus/models/version_2/images/test/gray_kitchen.NM.039.jpg\n",
      "/happy-walrus/models/version_2/images/test/large_kitchen.EH.039.jpg\n",
      "/happy-walrus/models/version_2/images/test/cluttered_kitchen.NM.008.jpg\n",
      "/happy-walrus/models/version_2/images/test/blue_kitchen.NM.085.jpg\n",
      "/happy-walrus/models/version_2/images/test/kitchen_corners.NM.036.jpg\n",
      "/happy-walrus/models/version_2/images/test/blue_kitchen.NM.067.jpg\n",
      "/happy-walrus/models/version_2/images/test/pintrest_cluttered_kitchen.EH.059.jpg\n",
      "/happy-walrus/models/version_2/images/test/Indian_kitchen.NM.039.jpg\n",
      "/happy-walrus/models/version_2/images/test/small_kitchen.EH.053.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010372.jpg\n",
      "/happy-walrus/models/version_2/images/test/big_kitchen.NM.045.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010389.jpg\n",
      "/happy-walrus/models/version_2/images/test/large_kitchen.EH.010.jpg\n",
      "/happy-walrus/models/version_2/images/test/American_kitchen.NM.058.jpg\n",
      "/happy-walrus/models/version_2/images/test/hot_stovetop.NM.006.jpg\n",
      "/happy-walrus/models/version_2/images/test/gray_kitchen.NM.069.jpg\n",
      "/happy-walrus/models/version_2/images/test/small_kitchen.EH.037.jpg\n",
      "/happy-walrus/models/version_2/images/test/South_American_kitchen.EH.011.jpg\n",
      "/happy-walrus/models/version_2/images/test/black_kitchen.NM.007.jpg\n",
      "/happy-walrus/models/version_2/images/test/kitchen_apartment.NM.083.jpg\n",
      "/happy-walrus/models/version_2/images/test/clean_kitchen.NM.040.jpg\n",
      "/happy-walrus/models/version_2/images/test/pintrest_messy_kitchen.EH.025.jpg\n",
      "/happy-walrus/models/version_2/images/test/grey_kitchen.NM.071.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010318.jpg\n",
      "/happy-walrus/models/version_2/images/test/old_kitchen.EH.023.jpg\n",
      "/happy-walrus/models/version_2/images/test/large_kitchen.EH.002.jpg\n",
      "/happy-walrus/models/version_2/images/test/European_kitchen.NM.002.jpg\n",
      "/happy-walrus/models/version_2/images/test/grey_kitchen.NM.028.jpg\n",
      "/happy-walrus/models/version_2/images/test/tiny_kitchen.EH.055.jpg\n",
      "/happy-walrus/models/version_2/images/test/large_kitchen.EH.048.jpg\n",
      "/happy-walrus/models/version_2/images/test/messy_kitchen.EH.011.jpg\n",
      "/happy-walrus/models/version_2/images/test/cluttered_kitchen.NM.017.jpg\n",
      "/happy-walrus/models/version_2/images/test/European_kitchen.NM.027.jpg\n",
      "/happy-walrus/models/version_2/images/test/pintrest_clean_kitchen.EH.043.jpg\n",
      "/happy-walrus/models/version_2/images/test/boiling_water.NM.002.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010302.jpg\n",
      "/happy-walrus/models/version_2/images/test/kitchen_odd_angle.EH.049.jpg\n",
      "/happy-walrus/models/version_2/images/test/cooking_with_family.NM.018.jpg\n",
      "/happy-walrus/models/version_2/images/test/orange_kitchen.EH.076.jpg\n",
      "/happy-walrus/models/version_2/images/test/tiny_kitchen.EH.081.jpg\n",
      "/happy-walrus/models/version_2/images/test/large_kitchen.EH.040.jpg\n",
      "/happy-walrus/models/version_2/images/test/grey_kitchen.NM.035.jpg\n",
      "/happy-walrus/models/version_2/images/test/pintrest_messy_kitchen.EH.023.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010407.jpg\n",
      "/happy-walrus/models/version_2/images/test/black_kitchen.NM.023.jpg\n",
      "/happy-walrus/models/version_2/images/test/small_kitchen.EH.024.jpg\n",
      "/happy-walrus/models/version_2/images/test/cluttered_kitchen.NM.030.jpg\n",
      "/happy-walrus/models/version_2/images/test/clean_kitchen.NM.061.jpg\n",
      "/happy-walrus/models/version_2/images/test/blue_kitchen.NM.038.jpg\n",
      "/happy-walrus/models/version_2/images/test/American_kitchen.NM.030.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010720.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010401.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010671.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010553.jpg\n",
      "/happy-walrus/models/version_2/images/test/large_kitchen.EH.008.jpg\n",
      "/happy-walrus/models/version_2/images/test/cluttered_kitchen.NM.038.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010485.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010497.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010675.jpg\n",
      "/happy-walrus/models/version_2/images/test/purple_kitchen.EH.001.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00000633.jpg\n",
      "/happy-walrus/models/version_2/images/test/large_kitchen.EH.015.jpg\n",
      "/happy-walrus/models/version_2/images/test/kitchen_corners.NM.029.jpg\n",
      "/happy-walrus/models/version_2/images/test/kitchen_apartment.NM.055.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010703.jpg\n",
      "/happy-walrus/models/version_2/images/test/blue_kitchen.NM.049.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010440.jpg\n",
      "/happy-walrus/models/version_2/images/test/European_kitchen.NM.032.jpg\n",
      "/happy-walrus/models/version_2/images/test/Indian_kitchen.NM.021.jpg\n",
      "/happy-walrus/models/version_2/images/test/grey_kitchen.NM.023.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010432.jpg\n",
      "/happy-walrus/models/version_2/images/test/old_kitchen.EH.020.jpg\n",
      "/happy-walrus/models/version_2/images/test/pintrest_clean_kitchen.EH.004.jpg\n",
      "/happy-walrus/models/version_2/images/test/white_kitchen.EH.040.jpg\n",
      "/happy-walrus/models/version_2/images/test/cluttered_kitchen.NM.040.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/happy-walrus/models/version_2/images/test/ADE_train_00010155.jpg\n",
      "/happy-walrus/models/version_2/images/test/orange_kitchen.EH.055.jpg\n",
      "/happy-walrus/models/version_2/images/test/South_American_kitchen.EH.006.jpg\n",
      "/happy-walrus/models/version_2/images/test/black_kitchen.NM.058.jpg\n",
      "/happy-walrus/models/version_2/images/test/boiling_water.NM.001.jpg\n",
      "/happy-walrus/models/version_2/images/test/Indian_kitchen.NM.007.jpg\n",
      "/happy-walrus/models/version_2/images/test/clean_kitchen.NM.056.jpg\n",
      "/happy-walrus/models/version_2/images/test/old_kitchen.EH.036.jpg\n",
      "/happy-walrus/models/version_2/images/test/kitchen_apartment.NM.004.jpg\n",
      "/happy-walrus/models/version_2/images/test/pintrest_messy_kitchen.EH.011.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010280.jpg\n",
      "/happy-walrus/models/version_2/images/test/large_kitchen.EH.003.jpg\n",
      "/happy-walrus/models/version_2/images/test/blue_kitchen.NM.063.jpg\n",
      "/happy-walrus/models/version_2/images/test/old_kitchen.EH.004.jpg\n",
      "/happy-walrus/models/version_2/images/test/messy_kitchen.EH.014.jpg\n",
      "/happy-walrus/models/version_2/images/test/large_kitchen.EH.027.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010472.jpg\n",
      "/happy-walrus/models/version_2/images/test/messy_kitchen.EH.018.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010216.jpg\n",
      "/happy-walrus/models/version_2/images/test/white_kitchen.EH.071.jpg\n",
      "/happy-walrus/models/version_2/images/test/Indian_kitchen.NM.022.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010723.jpg\n",
      "/happy-walrus/models/version_2/images/test/gray_kitchen.NM.024.jpg\n",
      "/happy-walrus/models/version_2/images/test/large_kitchen.EH.086.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010695.jpg\n",
      "/happy-walrus/models/version_2/images/test/large_kitchen.EH.066.jpg\n",
      "/happy-walrus/models/version_2/images/test/white_kitchen.EH.077.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010166.jpg\n",
      "/happy-walrus/models/version_2/images/test/pintrest_clean_kitchen.EH.015.jpg\n",
      "/happy-walrus/models/version_2/images/test/kitchen_unsafe.EH.034.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010449.jpg\n",
      "/happy-walrus/models/version_2/images/test/kitchen_unsafe.EH.002.jpg\n",
      "/happy-walrus/models/version_2/images/test/cluttered_kitchen.NM.014.jpg\n",
      "/happy-walrus/models/version_2/images/test/gray_kitchen.NM.068.jpg\n",
      "/happy-walrus/models/version_2/images/test/pintrest_clean_kitchen.EH.029.jpg\n",
      "/happy-walrus/models/version_2/images/test/kitchen_apartment.NM.079.jpg\n",
      "/happy-walrus/models/version_2/images/test/hot_stove.NM.008.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010438.jpg\n",
      "/happy-walrus/models/version_2/images/test/pintrest_cluttered_kitchen.EH.003.jpg\n",
      "/happy-walrus/models/version_2/images/test/blue_kitchen.NM.089.jpg\n",
      "/happy-walrus/models/version_2/images/test/linoleum_kitchen.EH.021.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010550.jpg\n",
      "/happy-walrus/models/version_2/images/test/green_kitchen.NM.045.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010223.jpg\n",
      "/happy-walrus/models/version_2/images/test/pintrest_clean_kitchen.EH.040.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010167.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010399.jpg\n",
      "/happy-walrus/models/version_2/images/test/large_kitchen.EH.035.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010656.jpg\n",
      "/happy-walrus/models/version_2/images/test/cluttered_kitchen.NM.034.jpg\n",
      "/happy-walrus/models/version_2/images/test/grey_kitchen.NM.050.jpg\n",
      "/happy-walrus/models/version_2/images/test/big_kitchen.NM.060.jpg\n",
      "/happy-walrus/models/version_2/images/test/pintrest_cluttered_kitchen.EH.048.jpg\n",
      "/happy-walrus/models/version_2/images/test/blue_kitchen.NM.045.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010165.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010359.jpg\n",
      "/happy-walrus/models/version_2/images/test/pintrest_messy_kitchen.EH.047.jpg\n",
      "/happy-walrus/models/version_2/images/test/boiling_water.NM.011.jpg\n",
      "/happy-walrus/models/version_2/images/test/old_kitchen.EH.065.jpg\n",
      "/happy-walrus/models/version_2/images/test/linoleum_kitchen.EH.005.jpg\n",
      "/happy-walrus/models/version_2/images/test/linoleum_kitchen.EH.065.jpg\n",
      "/happy-walrus/models/version_2/images/test/white_kitchen.EH.044.jpg\n",
      "/happy-walrus/models/version_2/images/test/orange_kitchen.EH.013.jpg\n",
      "/happy-walrus/models/version_2/images/test/black_kitchen.NM.069.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010701.jpg\n",
      "/happy-walrus/models/version_2/images/test/grey_kitchen.NM.019.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010325.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010586.jpg\n",
      "/happy-walrus/models/version_2/images/test/grey_kitchen.NM.042.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010164.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010524.jpg\n",
      "/happy-walrus/models/version_2/images/test/large_kitchen.EH.025.jpg\n",
      "/happy-walrus/models/version_2/images/test/gray_kitchen.NM.057.jpg\n",
      "/happy-walrus/models/version_2/images/test/white_kitchen.EH.009.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010554.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010163.jpg\n",
      "/happy-walrus/models/version_2/images/test/kitchen_apartment.NM.081.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010746.jpg\n",
      "/happy-walrus/models/version_2/images/test/American_kitchen.NM.011.jpg\n",
      "/happy-walrus/models/version_2/images/test/old_kitchen.EH.030.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010371.jpg\n",
      "/happy-walrus/models/version_2/images/test/orange_kitchen.EH.064.jpg\n",
      "/happy-walrus/models/version_2/images/test/European_kitchen.NM.066.jpg\n",
      "/happy-walrus/models/version_2/images/test/orange_kitchen.EH.063.jpg\n",
      "/happy-walrus/models/version_2/images/test/large_kitchen.EH.068.jpg\n",
      "/happy-walrus/models/version_2/images/test/orange_kitchen.EH.066.jpg\n",
      "/happy-walrus/models/version_2/images/test/purple_kitchen.EH.008.jpg\n",
      "/happy-walrus/models/version_2/images/test/large_kitchen.EH.031.jpg\n",
      "/happy-walrus/models/version_2/images/test/purple_kitchen.EH.012.jpg\n",
      "/happy-walrus/models/version_2/images/test/kitchen_danger.NM.002.jpg\n",
      "/happy-walrus/models/version_2/images/test/red_kitchen.EH.010.jpg\n",
      "/happy-walrus/models/version_2/images/test/clean_kitchen.NM.046.jpg\n",
      "/happy-walrus/models/version_2/images/test/pintrest_cluttered_kitchen.EH.010.jpg\n",
      "/happy-walrus/models/version_2/images/test/linoleum_kitchen.EH.018.jpg\n",
      "/happy-walrus/models/version_2/images/test/kitchen_odd_angle.EH.013.jpg\n",
      "/happy-walrus/models/version_2/images/test/hot_stovetop.NM.015.jpg\n",
      "/happy-walrus/models/version_2/images/test/black_kitchen.NM.029.jpg\n",
      "/happy-walrus/models/version_2/images/test/pintrest_messy_kitchen.EH.048.jpg\n",
      "/happy-walrus/models/version_2/images/test/American_kitchen.NM.046.jpg\n",
      "/happy-walrus/models/version_2/images/test/green_kitchen.NM.049.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010612.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010744.jpg\n",
      "/happy-walrus/models/version_2/images/test/big_kitchen.NM.011.jpg\n",
      "/happy-walrus/models/version_2/images/test/linoleum_kitchen.EH.043.jpg\n",
      "/happy-walrus/models/version_2/images/test/messy_kitchen.EH.046.jpg\n",
      "/happy-walrus/models/version_2/images/test/kitchen_corners.NM.023.jpg\n",
      "/happy-walrus/models/version_2/images/test/pintrest_cluttered_kitchen.EH.013.jpg\n",
      "/happy-walrus/models/version_2/images/test/messy_kitchen.EH.048.jpg\n",
      "/happy-walrus/models/version_2/images/test/kitchen_odd_angle.EH.022.jpg\n",
      "/happy-walrus/models/version_2/images/test/gray_kitchen.NM.005.jpg\n",
      "/happy-walrus/models/version_2/images/test/clean_kitchen.NM.030.jpg\n",
      "/happy-walrus/models/version_2/images/test/American_kitchen.NM.039.jpg\n",
      "/happy-walrus/models/version_2/images/test/kitchen_odd_angle.EH.066.jpg\n",
      "/happy-walrus/models/version_2/images/test/large_kitchen.EH.057.jpg\n",
      "/happy-walrus/models/version_2/images/test/kitchen_corners.NM.069.jpg\n",
      "/happy-walrus/models/version_2/images/test/clean_kitchen.NM.011.jpg\n",
      "/happy-walrus/models/version_2/images/test/European_kitchen.NM.004.jpg\n",
      "/happy-walrus/models/version_2/images/test/red_kitchen.EH.038.jpg\n",
      "/happy-walrus/models/version_2/images/test/black_kitchen.NM.021.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/happy-walrus/models/version_2/images/test/kitchen_apartment.NM.070.jpg\n",
      "/happy-walrus/models/version_2/images/test/purple_kitchen.EH.009.jpg\n",
      "/happy-walrus/models/version_2/images/test/green_kitchen.NM.012.jpg\n",
      "/happy-walrus/models/version_2/images/test/red_kitchen.EH.015.jpg\n",
      "/happy-walrus/models/version_2/images/test/small_kitchen.EH.060.jpg\n",
      "/happy-walrus/models/version_2/images/test/green_kitchen.NM.029.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010242.jpg\n",
      "/happy-walrus/models/version_2/images/test/kitchen_danger.NM.021.jpg\n",
      "/happy-walrus/models/version_2/images/test/pintrest_cluttered_kitchen.EH.066.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010281.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00000615.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010513.jpg\n",
      "/happy-walrus/models/version_2/images/test/clean_kitchen.NM.062.jpg\n",
      "/happy-walrus/models/version_2/images/test/linoleum_kitchen.EH.056.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010291.jpg\n",
      "/happy-walrus/models/version_2/images/test/kitchen_corners.NM.057.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010638.jpg\n",
      "/happy-walrus/models/version_2/images/test/kitchen_apartment.NM.006.jpg\n",
      "/happy-walrus/models/version_2/images/test/American_kitchen.NM.010.jpg\n",
      "/happy-walrus/models/version_2/images/test/American_kitchen.NM.001.jpg\n",
      "/happy-walrus/models/version_2/images/test/white_kitchen.EH.089.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010283.jpg\n",
      "/happy-walrus/models/version_2/images/test/linoleum_kitchen.EH.045.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010669.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010456.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010654.jpg\n",
      "/happy-walrus/models/version_2/images/test/purple_kitchen.EH.046.jpg\n",
      "/happy-walrus/models/version_2/images/test/black_kitchen.NM.048.jpg\n",
      "/happy-walrus/models/version_2/images/test/cluttered_kitchen.NM.031.jpg\n",
      "/happy-walrus/models/version_2/images/test/blue_kitchen.NM.084.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010560.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010507.jpg\n",
      "/happy-walrus/models/version_2/images/test/large_kitchen.EH.067.jpg\n",
      "/happy-walrus/models/version_2/images/test/blue_kitchen.NM.052.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00000624.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010535.jpg\n",
      "/happy-walrus/models/version_2/images/test/pintrest_clean_kitchen.EH.041.jpg\n",
      "/happy-walrus/models/version_2/images/test/white_kitchen.EH.057.jpg\n",
      "/happy-walrus/models/version_2/images/test/purple_kitchen.EH.037.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010662.jpg\n",
      "/happy-walrus/models/version_2/images/test/tiny_kitchen.EH.074.jpg\n",
      "/happy-walrus/models/version_2/images/test/orange_kitchen.EH.035.jpg\n",
      "/happy-walrus/models/version_2/images/test/black_kitchen.NM.004.jpg\n",
      "/happy-walrus/models/version_2/images/test/ADE_train_00010233.jpg\n"
     ]
    }
   ],
   "source": [
    "!python {MAIN_PATH}Object_detection_image.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -rq {MAIN_PATH}evaluation/bboxes.zip {MAIN_PATH}evaluation/bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make inferences on test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /models/research/object_detection/inference/infer_detections.py:96: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\r\n",
      "\r\n",
      "WARNING:tensorflow:From /models/research/object_detection/inference/infer_detections.py:57: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\r\n",
      "\r\n",
      "W1126 14:32:56.438692 140041540327232 module_wrapper.py:139] From /models/research/object_detection/inference/infer_detections.py:57: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\r\n",
      "\r\n",
      "WARNING:tensorflow:From /models/research/object_detection/inference/infer_detections.py:57: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\r\n",
      "\r\n",
      "W1126 14:32:56.438922 140041540327232 module_wrapper.py:139] From /models/research/object_detection/inference/infer_detections.py:57: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\r\n",
      "\r\n",
      "WARNING:tensorflow:From /models/research/object_detection/inference/infer_detections.py:65: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n",
      "\r\n",
      "W1126 14:32:56.439109 140041540327232 module_wrapper.py:139] From /models/research/object_detection/inference/infer_detections.py:65: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n",
      "\r\n",
      "2019-11-26 14:32:56.439965: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n",
      "2019-11-26 14:32:56.440007: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\r\n",
      "2019-11-26 14:32:56.440034: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (5df86dfc3c25): /proc/driver/nvidia/version does not exist\r\n",
      "2019-11-26 14:32:56.440338: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n",
      "2019-11-26 14:32:56.448214: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100060000 Hz\r\n",
      "2019-11-26 14:32:56.449112: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5062b40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n",
      "2019-11-26 14:32:56.449152: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n",
      "WARNING:tensorflow:From /models/research/object_detection/inference/infer_detections.py:68: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\r\n",
      "\r\n",
      "W1126 14:32:56.451408 140041540327232 module_wrapper.py:139] From /models/research/object_detection/inference/infer_detections.py:68: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\r\n",
      "\r\n",
      "INFO:tensorflow:Reading input from 1 files\r\n",
      "I1126 14:32:56.451569 140041540327232 infer_detections.py:68] Reading input from 1 files\r\n",
      "WARNING:tensorflow:From /models/research/object_detection/inference/detection_inference.py:35: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W1126 14:32:56.452336 140041540327232 deprecation.py:323] From /models/research/object_detection/inference/detection_inference.py:35: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:277: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "W1126 14:32:56.459507 140041540327232 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:277: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\r\n",
      "W1126 14:32:56.459801 140041540327232 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\r\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:112: RefVariable.count_up_to (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Prefer Dataset.range instead.\r\n",
      "W1126 14:32:56.463208 140041540327232 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:112: RefVariable.count_up_to (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Prefer Dataset.range instead.\r\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py:2522: count_up_to (from tensorflow.python.ops.state_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Prefer Dataset.range instead.\r\n",
      "W1126 14:32:56.463395 140041540327232 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py:2522: count_up_to (from tensorflow.python.ops.state_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Prefer Dataset.range instead.\r\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W1126 14:32:56.466496 140041540327232 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "W1126 14:32:56.467661 140041540327232 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "To construct input pipelines, use the `tf.data` module.\r\n",
      "WARNING:tensorflow:From /models/research/object_detection/inference/detection_inference.py:37: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\r\n",
      "W1126 14:32:56.472275 140041540327232 deprecation.py:323] From /models/research/object_detection/inference/detection_inference.py:37: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\r\n",
      "WARNING:tensorflow:From /models/research/object_detection/inference/detection_inference.py:39: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\r\n",
      "\r\n",
      "W1126 14:32:56.473643 140041540327232 module_wrapper.py:139] From /models/research/object_detection/inference/detection_inference.py:39: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\r\n",
      "\r\n",
      "WARNING:tensorflow:From /models/research/object_detection/inference/detection_inference.py:43: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\r\n",
      "\r\n",
      "W1126 14:32:56.473851 140041540327232 module_wrapper.py:139] From /models/research/object_detection/inference/detection_inference.py:43: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\r\n",
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading graph and building model...\n",
      "I1126 14:32:56.528734 140041540327232 infer_detections.py:71] Reading graph and building model...\n",
      "WARNING:tensorflow:From /models/research/object_detection/inference/detection_inference.py:68: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "W1126 14:32:56.529096 140041540327232 module_wrapper.py:139] From /models/research/object_detection/inference/detection_inference.py:68: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From /models/research/object_detection/inference/detection_inference.py:70: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\n",
      "\n",
      "W1126 14:32:56.625202 140041540327232 module_wrapper.py:139] From /models/research/object_detection/inference/detection_inference.py:70: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\n",
      "\n",
      "WARNING:tensorflow:From /models/research/object_detection/inference/detection_inference.py:76: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1126 14:32:57.404529 140041540327232 module_wrapper.py:139] From /models/research/object_detection/inference/detection_inference.py:76: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "INFO:tensorflow:Running inference and writing output to /happy-walrus/models/version_3/evaluation/autoaugm_inceptionv2/test_detections.tfrecord-00000-of-00001\n",
      "I1126 14:32:57.427958 140041540327232 infer_detections.py:77] Running inference and writing output to /happy-walrus/models/version_3/evaluation/autoaugm_inceptionv2/test_detections.tfrecord-00000-of-00001\n",
      "WARNING:tensorflow:From /models/research/object_detection/inference/infer_detections.py:78: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.\n",
      "\n",
      "W1126 14:32:57.428342 140041540327232 module_wrapper.py:139] From /models/research/object_detection/inference/infer_detections.py:78: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /models/research/object_detection/inference/infer_detections.py:79: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "W1126 14:32:57.582319 140041540327232 deprecation.py:323] From /models/research/object_detection/inference/infer_detections.py:79: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /models/research/object_detection/inference/infer_detections.py:80: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n",
      "W1126 14:32:57.583480 140041540327232 module_wrapper.py:139] From /models/research/object_detection/inference/infer_detections.py:80: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From /models/research/object_detection/inference/infer_detections.py:84: The name tf.logging.log_every_n is deprecated. Please use tf.compat.v1.logging.log_every_n instead.\n",
      "\n",
      "W1126 14:32:57.583979 140041540327232 module_wrapper.py:139] From /models/research/object_detection/inference/infer_detections.py:84: The name tf.logging.log_every_n is deprecated. Please use tf.compat.v1.logging.log_every_n instead.\n",
      "\n",
      "INFO:tensorflow:Processed 0 images...\n",
      "I1126 14:32:57.584159 140041540327232 infer_detections.py:85] Processed 0 images...\n",
      "WARNING:tensorflow:From /models/research/object_detection/inference/detection_inference.py:117: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1126 14:32:57.584379 140041540327232 module_wrapper.py:139] From /models/research/object_detection/inference/detection_inference.py:117: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "INFO:tensorflow:Processed 10 images...\n",
      "I1126 14:33:02.601148 140041540327232 infer_detections.py:85] Processed 10 images...\n",
      "INFO:tensorflow:Processed 20 images...\n",
      "I1126 14:33:06.072411 140041540327232 infer_detections.py:85] Processed 20 images...\n",
      "INFO:tensorflow:Processed 30 images...\n",
      "I1126 14:33:09.520705 140041540327232 infer_detections.py:85] Processed 30 images...\n",
      "INFO:tensorflow:Processed 40 images...\n",
      "I1126 14:33:13.045996 140041540327232 infer_detections.py:85] Processed 40 images...\n",
      "INFO:tensorflow:Processed 50 images...\n",
      "I1126 14:33:16.483661 140041540327232 infer_detections.py:85] Processed 50 images...\n",
      "INFO:tensorflow:Processed 60 images...\n",
      "I1126 14:33:21.336178 140041540327232 infer_detections.py:85] Processed 60 images...\n",
      "INFO:tensorflow:Processed 70 images...\n",
      "I1126 14:33:24.945199 140041540327232 infer_detections.py:85] Processed 70 images...\n",
      "INFO:tensorflow:Processed 80 images...\n",
      "I1126 14:33:28.370135 140041540327232 infer_detections.py:85] Processed 80 images...\n",
      "INFO:tensorflow:Processed 90 images...\n",
      "I1126 14:33:31.962119 140041540327232 infer_detections.py:85] Processed 90 images...\n",
      "INFO:tensorflow:Processed 100 images...\n",
      "I1126 14:33:35.479641 140041540327232 infer_detections.py:85] Processed 100 images...\n",
      "INFO:tensorflow:Processed 110 images...\n",
      "I1126 14:33:39.096459 140041540327232 infer_detections.py:85] Processed 110 images...\n",
      "INFO:tensorflow:Processed 120 images...\n",
      "I1126 14:33:42.883379 140041540327232 infer_detections.py:85] Processed 120 images...\n",
      "INFO:tensorflow:Processed 130 images...\n",
      "I1126 14:33:48.014605 140041540327232 infer_detections.py:85] Processed 130 images...\n",
      "INFO:tensorflow:Processed 140 images...\n",
      "I1126 14:33:51.652675 140041540327232 infer_detections.py:85] Processed 140 images...\n",
      "INFO:tensorflow:Processed 150 images...\n",
      "I1126 14:33:55.164114 140041540327232 infer_detections.py:85] Processed 150 images...\n",
      "INFO:tensorflow:Processed 160 images...\n",
      "I1126 14:33:59.113274 140041540327232 infer_detections.py:85] Processed 160 images...\n",
      "INFO:tensorflow:Processed 170 images...\n",
      "I1126 14:34:03.240743 140041540327232 infer_detections.py:85] Processed 170 images...\n",
      "INFO:tensorflow:Processed 180 images...\n",
      "I1126 14:34:07.994994 140041540327232 infer_detections.py:85] Processed 180 images...\n",
      "INFO:tensorflow:Processed 190 images...\n",
      "I1126 14:34:11.619831 140041540327232 infer_detections.py:85] Processed 190 images...\n",
      "INFO:tensorflow:Processed 200 images...\n",
      "I1126 14:34:15.857545 140041540327232 infer_detections.py:85] Processed 200 images...\n",
      "INFO:tensorflow:Processed 210 images...\n",
      "I1126 14:34:19.379266 140041540327232 infer_detections.py:85] Processed 210 images...\n",
      "INFO:tensorflow:Processed 220 images...\n",
      "I1126 14:34:23.024728 140041540327232 infer_detections.py:85] Processed 220 images...\n",
      "INFO:tensorflow:Processed 230 images...\n",
      "I1126 14:34:26.491742 140041540327232 infer_detections.py:85] Processed 230 images...\n",
      "INFO:tensorflow:Processed 240 images...\n",
      "I1126 14:34:29.997061 140041540327232 infer_detections.py:85] Processed 240 images...\n",
      "INFO:tensorflow:Processed 250 images...\n",
      "I1126 14:34:34.522523 140041540327232 infer_detections.py:85] Processed 250 images...\n",
      "INFO:tensorflow:Processed 260 images...\n",
      "I1126 14:34:38.434201 140041540327232 infer_detections.py:85] Processed 260 images...\n",
      "INFO:tensorflow:Processed 270 images...\n",
      "I1126 14:34:42.059858 140041540327232 infer_detections.py:85] Processed 270 images...\n",
      "INFO:tensorflow:Processed 280 images...\n",
      "I1126 14:34:45.916074 140041540327232 infer_detections.py:85] Processed 280 images...\n",
      "INFO:tensorflow:Finished processing records\n",
      "I1126 14:34:46.987265 140041540327232 infer_detections.py:92] Finished processing records\n"
     ]
    }
   ],
   "source": [
    "!python {OD_PATH}inference/infer_detections.py \\\n",
    "  --input_tfrecord_paths={MAIN_PATH}test.record \\\n",
    "  --output_tfrecord_path={EVAL_PATH}test_detections.tfrecord-00000-of-00001 \\\n",
    "  --inference_graph={GRAPH_PATH}frozen_inference_graph.pb \\\n",
    "  --discard_image_pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate performance metrics.\n",
    "- There are two documented bugs, solved implementing [this solution](https://github.com/tensorflow/models/issues/3252#issuecomment-363669586) and [this one](https://github.com/tensorflow/models/issues/5924#issuecomment-455081147).\n",
    "    - Replace line 162 in `metrics/offline_eval_map_corloc.py` for `input_config = configs['eval_input_configs'][0]`.\n",
    "    - Replace lines 47 to 49 in `metrics/tf_example_parser.py` with:\n",
    "        ```python\n",
    "            if tf_example.features.feature[self.field_name].HasField(\"bytes_list\"):\n",
    "                result = tf_example.features.feature[self.field_name].bytes_list.value\n",
    "                result = \"\".join([x if type(x)=='str' else x.decode('utf-8') for x in result])\n",
    "            else:\n",
    "                result = None\n",
    "            return result\n",
    "        ```\n",
    "\n",
    "- Adapt evaluation path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=5.35s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.86s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.001\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.005\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.024\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.024\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /models/research/object_detection/metrics/offline_eval_map_corloc.py:46: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "WARNING:tensorflow:From /models/research/object_detection/metrics/offline_eval_map_corloc.py:46: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "WARNING:tensorflow:From /models/research/object_detection/metrics/offline_eval_map_corloc.py:172: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
      "\n",
      "WARNING:tensorflow:From /models/research/object_detection/utils/config_util.py:236: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "W1126 17:54:34.542032 139952838006592 module_wrapper.py:139] From /models/research/object_detection/utils/config_util.py:236: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From /models/research/object_detection/metrics/offline_eval_map_corloc.py:105: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "W1126 17:54:34.544550 139952838006592 module_wrapper.py:139] From /models/research/object_detection/metrics/offline_eval_map_corloc.py:105: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "INFO:tensorflow:Processing file: /happy-walrus/models/version_3/evaluation/autoaugm_inceptionv2/test_detections.tfrecord-00000-of-00001\n",
      "I1126 17:54:34.544687 139952838006592 offline_eval_map_corloc.py:105] Processing file: /happy-walrus/models/version_3/evaluation/autoaugm_inceptionv2/test_detections.tfrecord-00000-of-00001\n",
      "WARNING:tensorflow:From /models/research/object_detection/metrics/offline_eval_map_corloc.py:107: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "W1126 17:54:34.544913 139952838006592 deprecation.py:323] From /models/research/object_detection/metrics/offline_eval_map_corloc.py:107: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "WARNING:tensorflow:From /models/research/object_detection/metrics/offline_eval_map_corloc.py:111: The name tf.logging.log_every_n is deprecated. Please use tf.compat.v1.logging.log_every_n instead.\n",
      "\n",
      "W1126 17:54:34.556197 139952838006592 module_wrapper.py:139] From /models/research/object_detection/metrics/offline_eval_map_corloc.py:111: The name tf.logging.log_every_n is deprecated. Please use tf.compat.v1.logging.log_every_n instead.\n",
      "\n",
      "INFO:tensorflow:Processed 0 images...\n",
      "I1126 17:54:34.556413 139952838006592 offline_eval_map_corloc.py:112] Processed 0 images...\n",
      "INFO:tensorflow:Performing evaluation on 283 images.\n",
      "I1126 17:54:35.442656 139952838006592 coco_evaluation.py:205] Performing evaluation on 283 images.\n",
      "INFO:tensorflow:Loading and preparing annotation results...\n",
      "I1126 17:54:35.444819 139952838006592 coco_tools.py:115] Loading and preparing annotation results...\n",
      "INFO:tensorflow:DONE (t=0.05s)\n",
      "I1126 17:54:35.497889 139952838006592 coco_tools.py:137] DONE (t=0.05s)\n",
      "INFO:tensorflow:Writing metrics.\n",
      "I1126 17:54:41.827113 139952838006592 offline_eval_map_corloc.py:142] Writing metrics.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OD_PATH='/models/research/object_detection/'\n",
    "MAIN_PATH='/happy-walrus/models/version_3/'\n",
    "IMG_PATH=${MAIN_PATH}images/\n",
    "MODEL_NAME=autoaugm_inceptionv2/\n",
    "TRAIN_PATH=${MAIN_PATH}training/${MODEL_NAME}\n",
    "EVAL_PATH=${MAIN_PATH}evaluation/${MODEL_NAME}\n",
    "GRAPH_PATH=${MAIN_PATH}inference_graph${MODEL_NAME}\n",
    "METRICS_PATH=${EVAL_PATH}eval_metrics/\n",
    "# Choose metrics set from:\n",
    "#   - coco_detection_metrics (multiple IoUs)\n",
    "#   - pascal_voc_detection_metrics (per-class)\n",
    "METRICS_SET='coco_detection_metrics'\n",
    "\n",
    "# NUM_SHARDS=1  # Set to NUM_GPUS if using the parallel evaluation script\n",
    "# MAIN_PATH='/happy-walrus/models/version_3/'\n",
    "# OD_PATH='/models/research/object_detection/'\n",
    "# EVAL_PATH=${MAIN_PATH}evaluation/eval_metrics/\n",
    "\n",
    "mkdir -p ${METRICS_PATH}\n",
    "\n",
    "echo \"\n",
    "label_map_path: '/happy-walrus/models/version_3/training/labelmap.pbtxt'\n",
    "tf_record_input_reader: { input_path: '${EVAL_PATH}test_detections.tfrecord-00000-of-00001' }\n",
    "\" > ${METRICS_PATH}test_input_config.pbtxt\n",
    "\n",
    "echo \"\n",
    "metrics_set: '${METRICS_SET}'\n",
    "\" > ${METRICS_PATH}test_eval_config.pbtxt\n",
    "\n",
    "python ${OD_PATH}metrics/offline_eval_map_corloc.py \\\n",
    "  --eval_dir=${METRICS_PATH} \\\n",
    "  --eval_config_path=${METRICS_PATH}test_eval_config.pbtxt \\\n",
    "  --input_config_path=${METRICS_PATH}test_input_config.pbtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PascalBoxes_Precision/mAP@0.5IOU                       0.00037501438874659976 \r\n",
      "PascalBoxes_PerformanceByCategory/AP@0.5IOU/cabinet    0.00047055844267115683 \r\n",
      "PascalBoxes_PerformanceByCategory/AP@0.5IOU/chair      0.0003943217665615142  \r\n",
      "PascalBoxes_PerformanceByCategory/AP@0.5IOU/countertop 2.767196118065942e-05  \r\n",
      "PascalBoxes_PerformanceByCategory/AP@0.5IOU/dishwasher 0.0                    \r\n",
      "PascalBoxes_PerformanceByCategory/AP@0.5IOU/outlet     0.0                    \r\n",
      "PascalBoxes_PerformanceByCategory/AP@0.5IOU/oven       0.000889829811023182   \r\n",
      "PascalBoxes_PerformanceByCategory/AP@0.5IOU/sofa       0.0                    \r\n",
      "PascalBoxes_PerformanceByCategory/AP@0.5IOU/stool      0.0                    \r\n",
      "PascalBoxes_PerformanceByCategory/AP@0.5IOU/stove      0.0006382272565852379  \r\n",
      "PascalBoxes_PerformanceByCategory/AP@0.5IOU/utensil    0.0013295346494442471  \r\n"
     ]
    }
   ],
   "source": [
    "# pascal_voc_detection_metrics\n",
    "!csvtool readable {EVAL_PATH}eval_metrics/metrics.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DetectionBoxes_Precision/mAP          0.00015217268320537238 \r\n",
      "DetectionBoxes_Precision/mAP@.50IOU   0.0005234425083178967  \r\n",
      "DetectionBoxes_Precision/mAP@.75IOU   3.643657641689191e-06  \r\n",
      "DetectionBoxes_Precision/mAP (small)  0.00015217268320537238 \r\n",
      "DetectionBoxes_Precision/mAP (medium) -1.0                   \r\n",
      "DetectionBoxes_Precision/mAP (large)  -1.0                   \r\n",
      "DetectionBoxes_Recall/AR@1            0.0007859667835428641  \r\n",
      "DetectionBoxes_Recall/AR@10           0.0053512012245757155  \r\n",
      "DetectionBoxes_Recall/AR@100          0.02444306841668646    \r\n",
      "DetectionBoxes_Recall/AR@100 (small)  0.02444306841668646    \r\n",
      "DetectionBoxes_Recall/AR@100 (medium) -1.0                   \r\n",
      "DetectionBoxes_Recall/AR@100 (large)  -1.0                   \r\n"
     ]
    }
   ],
   "source": [
    "# coco_detection_metrics\n",
    "!csvtool readable {EVAL_PATH}eval_metrics/metrics.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [State of the Art Object Detection - Medium](https://medium.com/@lessw/state-of-the-art-object-detection-use-these-top-3-data-augmentations-and-google-brains-optimal-57ac6d8d1de5)\n",
    "- [Learning Data Augmentation Strategies for Object Detection](https://arxiv.org/abs/1906.11172v1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
