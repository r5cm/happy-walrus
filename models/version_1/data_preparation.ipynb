{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T23:40:46.576468Z",
     "start_time": "2019-10-21T23:40:46.571467Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T18:24:37.656898Z",
     "start_time": "2019-10-21T18:24:37.617881Z"
    }
   },
   "source": [
    "# Split images (test-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T23:30:00.699954Z",
     "start_time": "2019-10-21T23:29:43.277084Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split annotations in training and validation\n",
    "root_path = '../../'\n",
    "data_src_path = root_path + 'data/images/version_1/'\n",
    "files = [re.sub(\"\\.xml\", \"\", f) for f in os.listdir(data_src_path + 'annotations/') if re.match(r\".*xml$\", f)]\n",
    "\n",
    "train_files, valid_files = train_test_split(files, train_size=0.9)\n",
    "split_files = {'training/': train_files,\n",
    "               'validation/': valid_files}\n",
    "\n",
    "# Split data into \n",
    "errors = []\n",
    "for split in split_files:\n",
    "    for file in split_files[split]:\n",
    "        for dtype, ext in [('annotations/', '.xml'), ('images/', '.jpg')]:\n",
    "            src = data_src_path + dtype + file + ext\n",
    "            dst = './data/' + split + dtype + file + ext\n",
    "            try:\n",
    "                shutil.copy(src, dst)\n",
    "            except Exception as err:\n",
    "                errors.append([err, file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pascal VOC to COCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7wrKXLy0FP5b"
   },
   "source": [
    "Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T23:57:44.691163Z",
     "start_time": "2019-10-21T23:57:44.672162Z"
    }
   },
   "outputs": [],
   "source": [
    "def pascal_to_coco(src_dir, output):\n",
    "    \"\"\"\n",
    "    Basic info\n",
    "    \"\"\"\n",
    "    # Info\n",
    "    info_dict = {'description': \"kitchens with stoves/ovens and chairs\",\n",
    "                 'url': \"http://happy-walrus.com\",\n",
    "                 'version': \"1.0\",\n",
    "                 'year': 2019,\n",
    "                 'contributer': \"Happy Walrus team\",\n",
    "                 'date_created': \"2019/10/14\"}\n",
    "    info = {'info': info_dict}\n",
    "    \n",
    "    # Add licenses as required as blobs and to `licences` list below\n",
    "    licenses_dict_0 = {\"url\": \"test_0\",\n",
    "                       \"id\": 0,\n",
    "                       \"name\": \"no license specified\"}\n",
    "\n",
    "    licenses_dict_1 = {\"url\": \"test_1\",\n",
    "                       \"id\": 1,\n",
    "                       \"name\": \"no license specified\"}\n",
    "\n",
    "    licenses = [licenses_dict_0, licenses_dict_1]\n",
    "    \n",
    "    # Categories\n",
    "    categories = []\n",
    "    objects = {1: \"stove/oven\", 2: \"chair\"}\n",
    "    inv_objects = {value: key for key, value in objects.items()}\n",
    "\n",
    "    for key, name in objects.items():\n",
    "        temp_dict = {\"supercategory\": \"NULL\",\n",
    "                     \"id\": key,\n",
    "                     \"name\": name}\n",
    "        categories.append(temp_dict)\n",
    "    \n",
    "    \"\"\"\n",
    "    Images and annotaions\n",
    "    \"\"\"\n",
    "    # Set file directory. Initialize empty lists and counters\n",
    "    directory = src_dir\n",
    "\n",
    "    images = []\n",
    "    annotations = []\n",
    "\n",
    "    j = 0\n",
    "    k = 0\n",
    "\n",
    "    # Iterate through file structure\n",
    "    files = [f for f in sorted(os.listdir(directory)) if f.endswith(\".xml\")]\n",
    "    for file in files:\n",
    "        filename = file\n",
    "        full_path = directory + \"/\" + filename\n",
    "\n",
    "        # Construct full file path, read XML tree object, set tree root, and obtain image size\n",
    "        tree = ET.parse(full_path)\n",
    "        root = tree.getroot()\n",
    "        size = root.find('size')\n",
    "\n",
    "        # Add requisite information for each image and append to list\n",
    "        image = {\"id\": int(j), \n",
    "                 \"license\": 0,\n",
    "                 \"coco_url\": \"NULL\",\n",
    "                 \"flick_url\": \"NULL\",\n",
    "                 \"width\": int(size.find('width').text),\n",
    "                 \"height\": int(size.find('height').text),\n",
    "                 \"file_name\": root.find('filename').text,\n",
    "                 \"date_captured\": \"NULL\"  \n",
    "        }\n",
    "\n",
    "        images.append(image)\n",
    "\n",
    "        # Iterate through each annotated 'object' in an image\n",
    "        for obj in root.findall('object'):\n",
    "\n",
    "            # If annotation is not object of interest, continue to next iteration\n",
    "            if obj.find('name').text not in ['chair', 'stove/oven']:\n",
    "                continue\n",
    "\n",
    "            # Extract bounding box coordinates and calculate width, height, and area\n",
    "            bndbox = obj.find('bndbox')\n",
    "\n",
    "            # A similar script I saw subtracts one from `xmin` and `ymin`. I don't understand why, so I haven't here\n",
    "            xmin = int(bndbox.find('xmin').text)\n",
    "            ymin = int(bndbox.find('ymin').text)\n",
    "            xmax = int(bndbox.find('xmax').text)\n",
    "            ymax = int(bndbox.find('ymax').text)\n",
    "\n",
    "            width = xmax - xmin\n",
    "            height = ymax - ymin\n",
    "            area = width * height\n",
    "\n",
    "            # Extract name (i.e., category) and use inverted dictionary to get category_id\n",
    "            name = obj.find('name').text\n",
    "            category_id = inv_objects[name]        \n",
    "\n",
    "            # Create annotation. Leave segmentation as empty list\n",
    "            annotation = {\"id\": k,\n",
    "                          \"category_id\": category_id,\n",
    "                          \"iscrowd\": 0,\n",
    "                          \"segmentation\": [], \n",
    "                          \"image_id\": j,\n",
    "                          \"area\": area,\n",
    "                          \"bbox\": [xmin, ymin, width, height]\n",
    "\n",
    "            }\n",
    "\n",
    "            annotations.append(annotation)\n",
    "\n",
    "            # Increment annotation count\n",
    "            k += 1 \n",
    "\n",
    "        # Increment image count\n",
    "        j += 1\n",
    "        \n",
    "    # Instantiate ordered dictionary, add fields in correct order, and dump to JSON format\n",
    "    all_items = OrderedDict()\n",
    "    all_items['info'] = info_dict\n",
    "    all_items['licenses'] = licenses\n",
    "    all_items['images'] = images\n",
    "    all_items['annotations'] = annotations\n",
    "    all_items['categories'] = categories\n",
    "\n",
    "    all_items_json = json.dumps(all_items)\n",
    "\n",
    "    # Write JSON file\n",
    "    with open(output, 'w') as output_file:\n",
    "        output_file.write(all_items_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T00:03:27.173227Z",
     "start_time": "2019-10-22T00:03:26.420223Z"
    }
   },
   "outputs": [],
   "source": [
    "pascal_to_coco('./data/validation/annotations', './data/validation/coco_annotations.json')\n",
    "pascal_to_coco('./data/training/annotations', './data/training/coco_annotations.json')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "pascal_to_coco.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
